{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from bert import nli\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "nli_labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "rumor_labels = [\"rumor\", \"truth\", \"unconfirmed claim\",\"nonrumor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "predict_fn = nli.get_nli_predict_fn(\n",
    "    bert_base_dir = '../../chinese-bert_chinese_wwm_L-12_H-768_A-12',\n",
    "    output_dir = '../../xnli_output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/miniconda3/envs/bert/lib/python3.6/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxy_rumors=pd.read_csv('../data/DXY_rumors.csv')\n",
    "dxy_rumors = dxy_rumors.drop(['_id','id','mainSummary','summary','sourceUrl','crawlTime','body'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_rumor(input_text):\n",
    "    text_b = input_text\n",
    "    x = np.array(model.encode([text_b]))\n",
    "    z = np.array(model.encode(dxy_rumors.title.values))\n",
    "    f=z.dot(x.transpose()).reshape(284)\n",
    "    p=softmax(f)\n",
    "    most_match_index = np.argmax(p)\n",
    "    text_a = re.sub('[？?]','',dxy_rumors.title.values[most_match_index])\n",
    "    label_a = dxy_rumors.rumorType.values[most_match_index]\n",
    "    #dxy_rumors.values[np.where(p>0.004)]\n",
    "    #print(text_a,rumor_labels[label_a])\n",
    "    examples = [\n",
    "        nli.InputExample(guid='prodiction-1',text_a=text_a,text_b=text_b,label='neutral')\n",
    "    ]\n",
    "    result = predict_fn(examples)\n",
    "    for (i, prediction) in enumerate(result):\n",
    "        probabilities = prediction['probabilities']\n",
    "        print(text_a,'->',text_b,': ',nli_labels[np.argmax(probabilities)])\n",
    "        if np.argmax(probabilities)==0:\n",
    "            rumor_labels = [\"truth\", \"rumor\", \"unconfirmed claim\",\"nonrumor\"]\n",
    "        else:\n",
    "            rumor_labels = [\"rumor\", \"truth\", \"unconfirmed claim\",\"nonrumor\"]\n",
    "        if np.argmax(probabilities)<2:\n",
    "            print(text_b,' is ',rumor_labels[label_a])\n",
    "        else:\n",
    "            print('Most related '+rumor_labels[label_a]+' is: ',text_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "钟南山院士被传染 -> 钟南山没被传染 :  contradiction\n",
      "钟南山没被传染  is  truth\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('钟南山没被传染')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "钟南山院士被传染 -> 钟南山被传染 :  entailment\n",
      "钟南山被传染  is  rumor\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('钟南山被传染')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吃奥司他韦可以预防新冠病毒 -> 奥司他韦可以用于预防新冠病毒 :  entailment\n",
      "奥司他韦可以用于预防新冠病毒  is  rumor\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('奥司他韦可以用于预防新冠病毒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双黄连可以预防新冠病毒 -> 吃双黄连不可以预防病毒 :  contradiction\n",
      "吃双黄连不可以预防病毒  is  truth\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('吃双黄连不可以预防病毒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "双黄连可以预防新冠病毒 -> 吃双黄连可以预防病毒 :  entailment\n",
      "吃双黄连可以预防病毒  is  rumor\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('吃双黄连可以预防病毒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "家畜家禽海鲜都不能吃了 -> 汉堡好吃 :  neutral\n",
      "Most related rumor is:  家畜家禽海鲜都不能吃了\n"
     ]
    }
   ],
   "source": [
    "detect_rumor('汉堡好吃')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert]",
   "language": "python",
   "name": "conda-env-bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
